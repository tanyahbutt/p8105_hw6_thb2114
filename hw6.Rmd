---
title: "P8105 Homework 6 - Tanya Butt (thb2114)"
output: github_document
---

```{r, echo = FALSE, message = FALSE}
library(tidyverse)
library(viridis)
library(mgcv)
library(modelr)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Loading and cleaning the birthweight dataset:
```{r, collapse = TRUE}
birthweight_df = 
  read_csv("birthweight.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    babysex = as.factor(babysex),
    malform = as.factor(malform),
    frace = as.factor(frace),
    mrace = as.factor(mrace)
  ) %>% 
  select(-c(pnumlbw, pnumsga)) 
```

Below I propose a regression model for birthweight, based on a hypothesized 
structure of specific factors. The factors included in the proposed model are 
based on published references of potential influences on infant birth weight.

These factors include: the baby's sex, gestational age in weeks, mother's  age 
and weight at delivery, and mother's weight gain during pregnancy (lbs).

```{r, collapse = TRUE}
fit = lm(bwt ~ babysex + gaweeks + momage + delwt  + wtgain, data = birthweight_df)

fit %>% broom::tidy()
```

Below is a plot of the model's residuals against fitted values:
```{r, collapse = TRUE}
birthweight_df %>% 
  add_residuals(fit) %>% 
  add_predictions(fit) %>% 
  ggplot(aes(x = pred, y = resid)) + geom_point()
```

I will now compare my model against two models below:
```{r, collapse = TRUE}
cv_df =
  crossv_mc(birthweight_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(
    fit_mod  = map(train, ~lm(bwt ~ babysex + gaweeks + momage + delwt + wtgain, 
                              data = .x)),
    fit_alt_mod_1 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    fit_alt_mod_2 = map(train, ~lm(bwt ~ bhead + blength + babysex + 
                              bhead*blength + bhead*babysex + blength*babysex +
                              bhead*blength*babysex, data = .x))) %>% 
  mutate(
    rmse_fit_mod = map2_dbl(fit_mod, test, ~rmse(model = .x, data = .y)),
    rmse_fit_alt_mod_1 = map2_dbl(fit_alt_mod_1, test, ~rmse(model = .x, data = .y)),
    rmse_fit_alt_mod_2 = map2_dbl(fit_alt_mod_2, test, ~rmse(model = .x, data = .y)))

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

It appears as though alternative model 2, which includes head circumference, 
length, sex, and all interactions between these variables, had the best 
predictive accuracy. Whether this model is the best to use remains to be 
determined, as model fitting requires balancing complexity with goodness of fit
and interpretability. 


## Problem 2

Loading the 2017 Central Park weather data:
```{r, collapse = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

set.seed(1)
```

Creating 5000 bootstrap samples:
```{r, collapse = TRUE}
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}

boot_straps = 
  tibble(
    strap_number = 1:5000,
    strap_sample = rerun(5000, boot_sample(weather_df))
  )

boot_straps
```

Determining the distribution of r-squared hat:
```{r, collapse = TRUE}
bootstrap_results_1 = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap_sample, -models) %>% 
  unnest(results)
  
bootstrap_results_1_graph = ggplot(bootstrap_results_1, aes(x = r.squared)) + 
  geom_density()

bootstrap_results_1_graph
```

The distribution has a bit of a heavy tail extending to the lower values, 
suggesting there may be outliers included in the bootstrap sample.

Below is a table of the 95% CI for r hat squared calculated using the 5000 
bootstrap samples:
```{r, collapse = TRUE}
bootstrap_results_1 %>% 
  summarize(
    ci_lower = quantile(r.squared, 0.025),
    ci_upper = quantile(r.squared, 0.975)) %>% 
  knitr::kable()
```

Determining the distribution of log(beta zero hat * beta one hat):
```{r, collapse = TRUE}
bootstrap_results_2 = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap_sample, -models) %>% 
  unnest(results) %>%
  select(strap_number, term, estimate) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>% 
  rename(beta_zero = `(Intercept)`) %>% 
  rename(beta_one = tmin) %>% 
  mutate(log_interaction = log(beta_zero*beta_one))

bootstrap_results_2_graph = ggplot(bootstrap_results_2, aes(x = log_interaction)) + 
  geom_density()
  
bootstrap_results_2_graph
```

The distribution appears closer to normal than the r-squared hat distribution. 
There is a bit of a "shoulder" in the upper values, which may be related to the
frequency of outliers in the bootstrap sample.

Below is a table of the 95% CI for the log(beta zero hat * beta one hat) 
calculated using the 5000 bootstrap samples:
```{r, collapse = TRUE}
bootstrap_results_2 %>% 
  summarize(
    ci_lower = quantile(log_interaction, 0.025),
    ci_upper = quantile(log_interaction, 0.975)) %>% 
  knitr::kable()
```


